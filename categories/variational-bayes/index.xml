<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>variational-bayes on James Totterdell</title>
    <link>https://jatotterdell.github.io/categories/variational-bayes/</link>
    <description>Recent content in variational-bayes on James Totterdell</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 25 May 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://jatotterdell.github.io/categories/variational-bayes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Logistic Regression Variational Approximation - I</title>
      <link>https://jatotterdell.github.io/post/2019/05/25/logistic-regression-variational-approximation/</link>
      <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jatotterdell.github.io/post/2019/05/25/logistic-regression-variational-approximation/</guid>
      <description>Background Approximation Bounds Bohning Jaakkola-Jordan Saul-Jordan  Examples Example 1 Example 2 Example 3 (divergence)  Summary Useful Identities References   Background Previously, I had worked through derivations of variational approximations for a linear regression model and proportional hazards exponential model with right-censoring. This post works through approximations for logistic regression models.
Some general references on variational approximations for logistic regression are Murphy (2012), Nolan and Wand (2017), and Wand (2017).</description>
    </item>
    
    <item>
      <title>Linear Regression Variational Approximation</title>
      <link>https://jatotterdell.github.io/post/2019/05/15/linear-regression-variational-approximation/</link>
      <pubDate>Wed, 15 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jatotterdell.github.io/post/2019/05/15/linear-regression-variational-approximation/</guid>
      <description>For reference, a derivation of a variational approximation for linear regression following (Ormerod 2008) and (Ormerod and Wand 2010).
Recall the evidence lower bound is given by \[ \begin{aligned} \ln p(y|\theta) &amp;amp;\geq \mathcal{L}(y|\theta;q) \\ &amp;amp;= \mathbb E_q[\ln p(y,\theta) - \ln q(\theta)] \\ &amp;amp;= \mathbb E_q[\ln p(y|\theta)] + \mathbb E_q[\ln p(\theta)] + \mathbb H_q[\theta]. \end{aligned} \]
The model we are interested in is \[ \begin{aligned} y|\beta,\sigma^2 &amp;amp;\sim N(X\beta, \sigma^2 I_n) \\ \beta &amp;amp;\sim N(\mu_0,\Sigma_0) \\ \sigma^2 &amp;amp;\sim IG(a_0, b_0).</description>
    </item>
    
    <item>
      <title>Variational Inference for Exponential Proportional Hazards Model</title>
      <link>https://jatotterdell.github.io/post/2019/03/28/variational-inference-for-exponential-proportional-hazards-model/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jatotterdell.github.io/post/2019/03/28/variational-inference-for-exponential-proportional-hazards-model/</guid>
      <description>Variational Approximations Variational Bayes is an approximate Bayesian inference method based on choosing an approximating density from some restricted class of densities by minimising the Kullback-Leibler divergence \[ \text{KL}(p\vert\vert q) = \int_\Omega \log\frac{p(\theta)}{q(\theta)} p(\theta)d\theta. \] In Bayesian inference, the density to be approximated is usually the posterior probability of some model parameter of interest \[ p(\theta|y) = \frac{p(\theta,y)}{p(y )}, \] and so the approximating density \(q\) is chosen as \[ q^\star(\theta) = \underset{q\in\mathcal{Q}}{\text{argmin }} \text{KL}(q\lvert\rvert p).</description>
    </item>
    
  </channel>
</rss>
